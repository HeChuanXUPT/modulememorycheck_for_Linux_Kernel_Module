diff -uparN linux-2.6.32.bak/arch/x86/mm/modulememcheck/Makefile linux-2.6.32/arch/x86/mm/modulememcheck/Makefile
--- linux-2.6.32.bak/arch/x86/mm/modulememcheck/Makefile	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32/arch/x86/mm/modulememcheck/Makefile	2014-03-24 16:36:10.829183709 +0800
@@ -0,0 +1,2 @@
+obj-y := mmcheck_output.o modulememcheck.o
+
diff -uparN linux-2.6.32.bak/arch/x86/mm/modulememcheck/mmcheck_output.c linux-2.6.32/arch/x86/mm/modulememcheck/mmcheck_output.c
--- linux-2.6.32.bak/arch/x86/mm/modulememcheck/mmcheck_output.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32/arch/x86/mm/modulememcheck/mmcheck_output.c	2014-03-25 09:47:43.906317527 +0800
@@ -0,0 +1,232 @@
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/dcache.h>
+#include <linux/debugfs.h>
+#include <linux/stat.h>	// file mode
+#include <linux/fs.h>	// struct file_operations
+#include <linux/uaccess.h>	// copy_*_user functions
+#include <linux/string.h>	// strlen
+//#include <linux/compiler.h> // unlikely
+#include <linux/vmalloc.h>
+
+#include <asm/modulememcheck.h>
+#include "mmcheck_output.h"
+
+MODULE_LICENSE("GPL");
+
+/*
+struct mmc_output {
+	char *buf;
+	size_t size;
+};
+*/
+
+#define MMC_OUTPUT_DIR_NANE "modulememcheck"
+#define MMC_OUTPUT_CONTROL_FILE_NANE "controller"
+struct dentry *mmc_dir = NULL,  *mmc_control_file = NULL;
+
+/* control file flag
+ * 0: simple information
+ * others: detail information */
+static int mmc_control = 0;
+
+//char test_buf[128];
+
+static ssize_t mmc_ctrol_read(struct file *file, char __user *buf, size_t count, loff_t *f_pos)
+{
+	if (*f_pos > 0) 
+		return 0;
+	*f_pos = 1;
+
+	if (mmc_control) {
+		if (copy_to_user(buf, "on\n", 3))
+			return -EFAULT;
+		else 
+			return 3;
+	} else {
+		if (copy_to_user(buf, "off\n", 4))
+			return -EFAULT;
+		else 
+			return 4;
+	}
+/*
+	int len;
+	len = strlen(test_buf) + 1;
+	if(*f_pos == len) return 0;
+	if(count + *f_pos > len)
+		count = len - *f_pos;
+	//printk("count test_buf = %d\n", count);
+	copy_to_user(buf, mmc_control: "on" ? "off", count);
+	*f_pos += count;
+	return 1;
+	*/
+}
+
+static ssize_t mmc_ctrol_write(struct file *f, const char __user *buf, size_t size, loff_t *l)
+{
+	char control[5];
+	if (size > 4)
+		return -EINVAL;
+
+	if (copy_from_user(control, buf, size)) {
+		return -EFAULT;
+	}
+
+	if (0 == strncmp(control, "on\n", 3)) {
+		mmc_control = 1;
+	} else if (0 == strncmp(control, "off\n", 4)) {
+		mmc_control = 0;
+	} else {
+		return -EINVAL;
+	}
+	return size;
+}
+
+static struct file_operations mmc_ctrol_ops = {
+	.owner = THIS_MODULE,
+	.read = mmc_ctrol_read,
+	.write = mmc_ctrol_write
+};
+
+static int mmc_output_create_ctrol_file(void)
+{
+	mmc_control_file = debugfs_create_file(MMC_OUTPUT_CONTROL_FILE_NANE, 
+			S_IRUGO | S_IWUGO, mmc_dir, NULL, &mmc_ctrol_ops);
+
+	if (mmc_control_file == NULL) {
+		printk("[mmc_out] failed to create a file under MMC_OUTPUT_DIR_NANE\n");
+		return -1;
+	} else {
+		return 0;
+	}
+}
+
+/************************************************/
+
+static int mmc_address_open(struct inode *inode, struct file* filp)
+{
+	filp->private_data = inode->i_private;
+	return nonseekable_open(inode, filp);
+}
+
+static ssize_t mmc_address_read(struct file *file, char __user *buf, size_t count, loff_t *f_pos)
+{
+	struct modulememcheck_alloc_obj *alloc_obj;
+	int i;
+	char *data;
+	char *tmp;
+
+	if (*f_pos > 0) 
+		return 0;
+	*f_pos = 1;
+
+	count = 0; 
+
+	alloc_obj = (struct modulememcheck_alloc_obj *)file->private_data;
+
+	if (alloc_obj == NULL) {
+		return -EINVAL;
+	}
+
+	data = vmalloc(1024);
+	tmp = kmalloc(128, GFP_KERNEL);
+	
+	sprintf(data, "address : 0x%lx\n", alloc_obj->address);
+	count += strlen(data);
+
+	/* need to fix, should locked */
+	if (mmc_control) {
+		struct mmc_process_info *process = NULL;
+		struct hlist_head *head;
+		struct hlist_node *node;
+		for (i = 0; i < MMC_PROCESS_TABLE_SIZE; i++) {
+			head = &alloc_obj->pro_objs[i];
+			hlist_for_each_entry(process, node, head, hlist) {
+				/* one process info */
+				sprintf(tmp, "[%s] pid : %d\n\
+						total read  times = %ld\n\
+						total write times = %ld\n\n",
+						process->symbol_name, 
+						process->pid, 
+						process->readtimes, 
+						process->writetimes);
+				strcat(data, tmp);
+				count += strlen(tmp);
+			}
+		}
+	}
+
+	if (copy_to_user(buf, data, count)) {
+			return -EFAULT;
+	}
+
+	vfree(data);
+	kfree(tmp);
+
+	return count;
+}
+
+static int mmc_address_release(struct inode *inode, struct file* filp)
+{
+	filp->private_data = NULL;
+	return 0;
+}
+
+static struct file_operations mmc_address_ops = {
+	.owner = THIS_MODULE,
+	.open = mmc_address_open,
+	.read = mmc_address_read,
+	.release = mmc_address_release
+};
+
+int mmc_output_create_file(struct modulememcheck_alloc_obj *alloc_obj)
+{
+	struct dentry *file;
+	char buf[16];
+	sprintf(buf, "0x%lx", alloc_obj->address);
+	file = debugfs_create_file(buf, 
+			S_IRUGO, mmc_dir, alloc_obj, &mmc_address_ops);
+
+	if (file == NULL) {
+		printk("[mmc_out] failed to create a file under MMC_OUTPUT_DIR_NANE\n");
+		return -1;
+	} else {
+		return 0;
+	}
+}
+
+int mmc_output_init(void)
+{
+	static int first = 1;
+	if (!first)
+		return 0;
+	first = 0;
+	printk("[mmc_out] __init mmc_output_init\n");
+	mmc_dir = debugfs_create_dir(MMC_OUTPUT_DIR_NANE, NULL);
+
+	if (unlikely(IS_ERR(mmc_dir))) {
+		printk("[mmc_out] debugfs is not supportd\n");
+		return -ENODEV;
+	}
+
+	if (unlikely(mmc_dir == NULL)) {
+		printk("[mmc_out] failed to create a directory in debugfs\n");
+		return -EINVAL;
+	}
+
+	if (mmc_output_create_ctrol_file() < 0) {
+		printk("failed to create control file\n");
+		return -EINVAL;
+	}
+
+	//d2 = debugfs_create_dir(d1_name, d1);
+	return 0;
+}
+
+void mmc_output_exit(void)
+{
+	debugfs_remove(mmc_control_file);
+	//debugfs_remove(d2);
+	debugfs_remove(mmc_dir);
+	printk("[mmc_out] __exit mmc_output_exit\n");
+}
diff -uparN linux-2.6.32.bak/arch/x86/mm/modulememcheck/mmcheck_output.h linux-2.6.32/arch/x86/mm/modulememcheck/mmcheck_output.h
--- linux-2.6.32.bak/arch/x86/mm/modulememcheck/mmcheck_output.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32/arch/x86/mm/modulememcheck/mmcheck_output.h	2014-03-24 17:54:52.713141793 +0800
@@ -0,0 +1,9 @@
+#ifndef ASM_X86_MMEMCHECK_OUTPUT_H
+#define ASM_X86_MMEMCHECK_OUTPUT_H
+
+#include <asm/modulememcheck.h>
+int mmc_output_init(void);
+void mmc_output_exit(void);
+int mmc_output_create_file(struct modulememcheck_alloc_obj *alloc_obj);
+
+#endif
diff -uparN linux-2.6.32.bak/arch/x86/mm/modulememcheck/modulememcheck.c linux-2.6.32/arch/x86/mm/modulememcheck/modulememcheck.c
--- linux-2.6.32.bak/arch/x86/mm/modulememcheck/modulememcheck.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32/arch/x86/mm/modulememcheck/modulememcheck.c	2014-03-25 09:51:27.492598775 +0800
@@ -0,0 +1,836 @@
+/**
+ * kmemcheck - a heavyweight memory checker for the linux kernel
+ * Copyright (C) 2007, 2008  Vegard Nossum <vegardno@ifi.uio.no>
+ * (With a lot of help from Ingo Molnar and Pekka Enberg.)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License (version 2) as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kallsyms.h>
+#include <linux/kernel.h>
+#include <linux/kmemcheck.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/page-flags.h>
+#include <linux/percpu.h>
+#include <linux/ptrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/hash.h>
+
+#include <asm/cacheflush.h>
+#include <asm/modulememcheck.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+
+#include "mmcheck_output.h"
+//#include "error.h"
+//#include "opcode.h"
+//#include "pte.h"
+//#include "selftest.h"
+//#include "shadow.h"
+//
+struct modulememcheck_alloc_obj *mmcheck_alloc_objs = NULL;
+
+static struct modulememcheck_alloc_obj *mmcheck_obj_alloc(void);
+static struct mmc_process_info *mmc_process_alloc(void);
+/*
+#ifdef CONFIG_KMEMCHECK_DISABLED_BY_DEFAULT
+#  define KMEMCHECK_ENABLED 0
+#endif
+
+#ifdef CONFIG_KMEMCHECK_ENABLED_BY_DEFAULT
+#  define KMEMCHECK_ENABLED 1
+#endif
+
+#ifdef CONFIG_KMEMCHECK_ONESHOT_BY_DEFAULT
+#  define KMEMCHECK_ENABLED 2
+#endif
+*/
+//int kmemcheck_enabled = KMEMCHECK_ENABLED;
+static pte_t *modulememcheck_pte_lookup(unsigned long address)
+{
+	pte_t *pte;
+	unsigned int level;
+
+	pte = lookup_address(address, &level);
+	if (!pte)
+		return NULL;
+	/*
+	if (level != PG_LEVEL_4K)
+		return NULL;
+	if (!pte_hidden(*pte))
+		return NULL;
+	*/
+
+	/* check if the pte is interested */
+	if (!mmcheck_alloc_objs || mmcheck_alloc_objs->pte != pte) {
+		return NULL;
+	}
+	return pte;
+}
+
+int __init modulememcheck_init(void)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * Limit SMP to use a single CPU. We rely on the fact that this code
+	 * runs before SMP is set up.
+	 */
+	if (setup_max_cpus > 1) {
+		printk(KERN_INFO
+			"modulememcheck: Limiting number of CPUs to 1.\n");
+		setup_max_cpus = 1;
+	}
+#endif
+
+	/*
+	if (!kmemcheck_selftest()) {
+		printk(KERN_INFO "kmemcheck: self-tests failed; disabling\n");
+		kmemcheck_enabled = 0;
+		return -EINVAL;
+	}
+	*/
+
+	printk(KERN_INFO "modulememcheck: Initialized\n");
+	return 0;
+}
+
+early_initcall(modulememcheck_init);
+
+/*
+ * We need to parse the kmemcheck= option before any memory is allocated.
+static int __init param_kmemcheck(char *str)
+{
+	if (!str)
+		return -EINVAL;
+
+	sscanf(str, "%d", &kmemcheck_enabled);
+	return 0;
+}
+
+early_param("kmemcheck", param_kmemcheck);
+ */
+
+static int modulememcheck_show_addr(unsigned long address)
+{
+	pte_t *pte;
+	printk("[MODULE MEM CHECK] : modulememcheck show addr %lx! \n", address);
+
+	pte = modulememcheck_pte_lookup(address);
+	if (!pte)
+		return 0;
+
+	set_pte(pte, __pte(pte_val(*pte) | _PAGE_PRESENT));
+	__flush_tlb_one(address);
+	return 1;
+}
+
+static int modulememcheck_hide_addr(unsigned long address)
+{
+	pte_t *pte;
+	int level;
+	printk("[MODULE MEM CHECK] : modulememcheck hide addr %lx! \n", address);
+	//pte = modulememcheck_pte_lookup(address);
+	pte = lookup_address(address, &level);
+	if (!pte)
+		return 0;
+
+	set_pte(pte, __pte(pte_val(*pte) & ~_PAGE_PRESENT));
+	__flush_tlb_one(address);
+	return 1;
+}
+
+static void modulememcheck_set_singlestep(struct pt_regs *regs)
+{
+	regs->flags |= X86_EFLAGS_TF;
+	regs->flags &= ~X86_EFLAGS_IF;
+}
+
+static void modulememcheck_clear_singlestep(struct pt_regs *regs)
+{
+	regs->flags &= ~X86_EFLAGS_TF;
+	regs->flags |= X86_EFLAGS_IF;
+}
+
+static struct mmc_process_info *mmc_process_lookup(int pid)
+{
+	struct mmc_process_info *process = NULL;
+	struct hlist_head *head;
+	struct hlist_node *node;
+
+	head = &mmcheck_alloc_objs->pro_objs[hash_ptr((void *)pid, MMC_PROCESS_HASH_BITS)];
+	hlist_for_each_entry(process, node, head, hlist) {
+		if (process->pid == pid)
+			break;
+	}
+	return process;
+}
+
+enum mmc_access_type {
+	MMC_READ, MMC_WRITE
+};
+
+static void mmc_access_record(struct pt_regs *regs, unsigned long addr, enum mmc_access_type type)
+{
+	struct mmc_process_info *process;
+	int pid = current->pid;
+
+	if (mmcheck_alloc_objs == NULL)
+		return ;
+
+	if ((process = mmc_process_lookup(pid)) == NULL) {
+		// new mmc_process_info	
+		struct hlist_head *head;
+		process = mmc_process_alloc();
+		process->symbol_name = (char *)kmalloc(KSYM_SYMBOL_LEN, GFP_KERNEL);
+		sprint_symbol(process->symbol_name, (unsigned long)__builtin_extract_return_addr((void *)regs->ip));	
+		head = &mmcheck_alloc_objs->pro_objs[hash_ptr((void *)pid, MMC_PROCESS_HASH_BITS)];
+		hlist_add_head(&process->hlist, head);
+	}
+
+	if (type == MMC_READ) {
+		process->readtimes++;
+	} else {
+		process->writetimes++;
+	}
+}
+
+bool modulememcheck_fault(struct pt_regs *regs, unsigned long address,
+	unsigned long error_code)
+{
+	pte_t *pte;
+
+	/*
+	 * XXX: Is it safe to assume that memory accesses from virtual 86
+	 * mode or non-kernel code segments will _never_ access kernel
+	 * memory (e.g. tracked pages)? For now, we need this to avoid
+	 * invoking kmemcheck for PnP BIOS calls.
+	 */
+	if((mmcheck_alloc_objs == NULL) || !regs )
+		return false;
+	if (regs->flags & X86_VM_MASK)
+		return false;
+	if (regs->cs != __KERNEL_CS)
+		return false;
+
+	printk("[MODULE MEM CHECK] : modulememcheck falut ! \n");
+	pte = modulememcheck_pte_lookup(address);
+	if (!pte)
+		return false;
+
+	if (error_code & 2) {
+		printk("[MODULE MEM CHECK] : WRITE ERROR ! \n");
+		//kmemcheck_access(regs, address, KMEMCHECK_WRITE);
+		mmc_access_record(regs, address, MMC_WRITE);
+	} else {
+		printk("[MODULE MEM CHECK] : READ ERROR ! \n");
+		mmc_access_record(regs, address, MMC_READ);
+		//kmemcheck_access(regs, address, KMEMCHECK_READ);
+	}
+
+	/* test print */
+#ifdef CONFIG_X86_32
+	dump_stack();
+	printk("EIP: [<%08lx>] ", regs->ip);
+	print_symbol("%s \n", regs->ip);
+#endif
+
+	// make the pte present
+	modulememcheck_show_addr(address);
+
+	// for single step
+	modulememcheck_set_singlestep(regs);
+
+	return true;
+}
+
+bool modulememcheck_trap(struct pt_regs *regs)
+{
+	// We're done. 
+	modulememcheck_hide_addr(mmcheck_alloc_objs->address);
+	modulememcheck_clear_singlestep(regs);
+	return true;
+}
+
+int modulememcheck_alloc(unsigned long address)
+{
+	pte_t *pte;
+	int level;
+	printk("[MODULE MEM CHECK] : modulememcheck alloc ! \n");
+
+	/* need to fix  */
+	mmc_output_init();
+
+	pte = lookup_address(address, &level);
+	if (!pte)
+		return 0;
+
+	mmcheck_alloc_objs = mmcheck_obj_alloc();
+	mmcheck_alloc_objs->address = address;	
+	mmcheck_alloc_objs->pte = pte;	
+
+	if (mmc_output_create_file(mmcheck_alloc_objs) < 0) {
+		printk("[MODULE MEM CHECK] : debugfs create alloc file failed\n");
+		return 0;
+	}
+
+	modulememcheck_hide_addr(address);
+	return 1;
+} 
+EXPORT_SYMBOL(modulememcheck_alloc);
+
+/* alloc and initialize */
+static struct modulememcheck_alloc_obj *mmcheck_obj_alloc(void)
+{
+	struct modulememcheck_alloc_obj *obj;
+	int i;
+	obj = (struct modulememcheck_alloc_obj *)kmalloc(sizeof(struct modulememcheck_alloc_obj), GFP_KERNEL);
+	if (obj == NULL) {
+		printk("[MODULE MEM CHECK] : not enough memory to malloc struct mmcheck_alloc!\n");
+		return NULL;
+	}
+	
+	for (i = 0; i < MMC_PROCESS_TABLE_SIZE; i++) {
+		INIT_HLIST_HEAD(&obj->pro_objs[i]);
+	}
+
+	return obj;
+}
+
+/* alloc and initialize */
+static struct mmc_process_info *mmc_process_alloc(void)
+{
+	struct mmc_process_info *process;
+	process = (struct mmc_process_info *)kmalloc(sizeof(struct mmc_process_info), GFP_KERNEL);
+	if (process == NULL) {
+		printk("[MODULE MEM CHECK] : not enough memory to alloc mmc_process_info\n");
+		return NULL;
+	}
+	process->readtimes = 0;
+	process->writetimes = 0;
+	return process;
+}
+
+/*
+struct kmemcheck_context {
+	bool busy;
+	int balance;
+
+	*
+	 * There can be at most two memory operands to an instruction, but
+	 * each address can cross a page boundary -- so we may need up to
+	 * four addresses that must be hidden/revealed for each fault.
+	 *
+	unsigned long addr[4];
+	unsigned long n_addrs;
+	unsigned long flags;
+
+	/ Data size of the instruction that caused a fault. /
+	unsigned int size;
+};
+*/
+//static DEFINE_PER_CPU(struct kmemcheck_context, kmemcheck_context);
+/*
+bool kmemcheck_active(struct pt_regs *regs)
+{
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+
+	return data->balance > 0;
+}
+*/
+/* Save an address that needs to be shown/hidden */
+/*
+static void kmemcheck_save_addr(unsigned long addr)
+{ 
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+
+	BUG_ON(data->n_addrs >= ARRAY_SIZE(data->addr));
+	data->addr[data->n_addrs++] = addr;
+}
+
+static unsigned int kmemcheck_show_all(void)
+{
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+	unsigned int i;
+	unsigned int n;
+
+	n = 0;
+	for (i = 0; i < data->n_addrs; ++i)
+		n += kmemcheck_show_addr(data->addr[i]);
+
+	return n;
+}
+
+static unsigned int kmemcheck_hide_all(void)
+{
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+	unsigned int i;
+	unsigned int n;
+
+	n = 0;
+	for (i = 0; i < data->n_addrs; ++i)
+		n += kmemcheck_hide_addr(data->addr[i]);
+
+	return n;
+}
+*/
+
+/*
+ * Called from the #PF handler.
+void kmemcheck_show(struct pt_regs *regs)
+{
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+
+	BUG_ON(!irqs_disabled());
+
+	if (unlikely(data->balance != 0)) {
+		kmemcheck_show_all();
+		kmemcheck_error_save_bug(regs);
+		data->balance = 0;
+		return;
+	}
+
+	*
+	 * None of the addresses actually belonged to kmemcheck. Note that
+	 * this is not an error.
+	 *
+	if (kmemcheck_show_all() == 0)
+		return;
+
+	++data->balance;
+
+	*
+	 * The IF needs to be cleared as well, so that the faulting
+	 * instruction can run "uninterrupted". Otherwise, we might take
+	 * an interrupt and start executing that before we've had a chance
+	 * to hide the page again.
+	 *
+	 * NOTE: In the rare case of multiple faults, we must not override
+	 * the original flags:
+	 *
+	if (!(regs->flags & X86_EFLAGS_TF))
+		data->flags = regs->flags;
+
+	regs->flags |= X86_EFLAGS_TF;
+	regs->flags &= ~X86_EFLAGS_IF;
+}
+ */
+
+/*
+ * Called from the #DB handler.
+ */
+/*
+void kmemcheck_hide(struct pt_regs *regs)
+ {
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+	int n;
+
+	BUG_ON(!irqs_disabled());
+
+	if (unlikely(data->balance != 1)) {
+		kmemcheck_show_all();
+		kmemcheck_error_save_bug(regs);
+		data->n_addrs = 0;
+		data->balance = 0;
+
+		if (!(data->flags & X86_EFLAGS_TF))
+			regs->flags &= ~X86_EFLAGS_TF;
+		if (data->flags & X86_EFLAGS_IF)
+			regs->flags |= X86_EFLAGS_IF;
+		return;
+	}
+
+	if (kmemcheck_enabled)
+		n = kmemcheck_hide_all();
+	else
+		n = kmemcheck_show_all();
+
+	if (n == 0)
+		return;
+
+	--data->balance;
+
+	data->n_addrs = 0;
+
+	if (!(data->flags & X86_EFLAGS_TF))
+		regs->flags &= ~X86_EFLAGS_TF;
+	if (data->flags & X86_EFLAGS_IF)
+		regs->flags |= X86_EFLAGS_IF;
+}
+
+void kmemcheck_show_pages(struct page *p, unsigned int n)
+{
+	unsigned int i;
+
+	for (i = 0; i < n; ++i) {
+		unsigned long address;
+		pte_t *pte;
+		unsigned int level;
+
+		address = (unsigned long) page_address(&p[i]);
+		pte = lookup_address(address, &level);
+		BUG_ON(!pte);
+		BUG_ON(level != PG_LEVEL_4K);
+
+		set_pte(pte, __pte(pte_val(*pte) | _PAGE_PRESENT));
+		set_pte(pte, __pte(pte_val(*pte) & ~_PAGE_HIDDEN));
+		__flush_tlb_one(address);
+	}
+}
+
+bool kmemcheck_page_is_tracked(struct page *p)
+{
+	/ This will also check the "hidden" flag of the PTE. /
+	return kmemcheck_pte_lookup((unsigned long) page_address(p));
+}
+
+void kmemcheck_hide_pages(struct page *p, unsigned int n)
+{
+	unsigned int i;
+
+	for (i = 0; i < n; ++i) {
+		unsigned long address;
+		pte_t *pte;
+		unsigned int level;
+
+		address = (unsigned long) page_address(&p[i]);
+		pte = lookup_address(address, &level);
+		BUG_ON(!pte);
+		BUG_ON(level != PG_LEVEL_4K);
+
+		set_pte(pte, __pte(pte_val(*pte) & ~_PAGE_PRESENT));
+		set_pte(pte, __pte(pte_val(*pte) | _PAGE_HIDDEN));
+		__flush_tlb_one(address);
+	}
+}
+*/
+
+/* Access may NOT cross page boundary */
+/*
+static void kmemcheck_read_strict(struct pt_regs *regs,
+	unsigned long addr, unsigned int size)
+{
+	void *shadow;
+	enum kmemcheck_shadow status;
+
+	shadow = kmemcheck_shadow_lookup(addr);
+	if (!shadow)
+		return;
+
+	kmemcheck_save_addr(addr);
+	status = kmemcheck_shadow_test(shadow, size);
+	if (status == KMEMCHECK_SHADOW_INITIALIZED)
+		return;
+
+	if (kmemcheck_enabled)
+		kmemcheck_error_save(status, addr, size, regs);
+
+	if (kmemcheck_enabled == 2)
+		kmemcheck_enabled = 0;
+
+	// Don't warn about it again. /
+	kmemcheck_shadow_set(shadow, size);
+}
+*/
+
+/*bool kmemcheck_is_obj_initialized(unsigned long addr, size_t size)
+{
+	enum kmemcheck_shadow status;
+	void *shadow;
+
+	shadow = kmemcheck_shadow_lookup(addr);
+	if (!shadow)
+		return true;
+
+	status = kmemcheck_shadow_test(shadow, size);
+
+	return status == KMEMCHECK_SHADOW_INITIALIZED;
+} 
+
+// Access may cross page boundary /
+static void kmemcheck_read(struct pt_regs *regs,
+	unsigned long addr, unsigned int size)
+{
+	unsigned long page = addr & PAGE_MASK;
+	unsigned long next_addr = addr + size - 1;
+	unsigned long next_page = next_addr & PAGE_MASK;
+
+	if (likely(page == next_page)) {
+		kmemcheck_read_strict(regs, addr, size);
+		return;
+	}
+
+	*
+	 * What we do is basically to split the access across the
+	 * two pages and handle each part separately. Yes, this means
+	 * that we may now see reads that are 3 + 5 bytes, for
+	 * example (and if both are uninitialized, there will be two
+	 * reports), but it makes the code a lot simpler.
+	 *
+	kmemcheck_read_strict(regs, addr, next_page - addr);
+	kmemcheck_read_strict(regs, next_page, next_addr - next_page);
+}
+
+static void kmemcheck_write_strict(struct pt_regs *regs,
+	unsigned long addr, unsigned int size)
+{
+	void *shadow;
+
+	shadow = kmemcheck_shadow_lookup(addr);
+	if (!shadow)
+		return;
+
+	kmemcheck_save_addr(addr);
+	kmemcheck_shadow_set(shadow, size);
+}
+
+static void kmemcheck_write(struct pt_regs *regs,
+	unsigned long addr, unsigned int size)
+{
+	unsigned long page = addr & PAGE_MASK;
+	unsigned long next_addr = addr + size - 1;
+	unsigned long next_page = next_addr & PAGE_MASK;
+
+	if (likely(page == next_page)) {
+		kmemcheck_write_strict(regs, addr, size);
+		return;
+	}
+
+	// See comment in kmemcheck_read(). /
+	kmemcheck_write_strict(regs, addr, next_page - addr);
+	kmemcheck_write_strict(regs, next_page, next_addr - next_page);
+}
+
+*/
+/*
+ * Copying is hard. We have two addresses, each of which may be split across
+ * a page (and each page will have different shadow addresses).
+static void kmemcheck_copy(struct pt_regs *regs,
+	unsigned long src_addr, unsigned long dst_addr, unsigned int size)
+{
+	uint8_t shadow[8];
+	enum kmemcheck_shadow status;
+
+	unsigned long page;
+	unsigned long next_addr;
+	unsigned long next_page;
+
+	uint8_t *x;
+	unsigned int i;
+	unsigned int n;
+
+	BUG_ON(size > sizeof(shadow));
+
+	page = src_addr & PAGE_MASK;
+	next_addr = src_addr + size - 1;
+	next_page = next_addr & PAGE_MASK;
+
+	if (likely(page == next_page)) {
+		/ Same page /
+		x = kmemcheck_shadow_lookup(src_addr);
+		if (x) {
+			kmemcheck_save_addr(src_addr);
+			for (i = 0; i < size; ++i)
+				shadow[i] = x[i];
+		} else {
+			for (i = 0; i < size; ++i)
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+		}
+	} else {
+		n = next_page - src_addr;
+		BUG_ON(n > sizeof(shadow));
+
+		/ First page /
+		x = kmemcheck_shadow_lookup(src_addr);
+		if (x) {
+			kmemcheck_save_addr(src_addr);
+			for (i = 0; i < n; ++i)
+				shadow[i] = x[i];
+		} else {
+			/ Not tracked /
+			for (i = 0; i < n; ++i)
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+		}
+
+		/ Second page /
+		x = kmemcheck_shadow_lookup(next_page);
+		if (x) {
+			kmemcheck_save_addr(next_page);
+			for (i = n; i < size; ++i)
+				shadow[i] = x[i - n];
+		} else {
+			/ Not tracked /
+			for (i = n; i < size; ++i)
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+		}
+	}
+
+	page = dst_addr & PAGE_MASK;
+	next_addr = dst_addr + size - 1;
+	next_page = next_addr & PAGE_MASK;
+
+	if (likely(page == next_page)) {
+		/ Same page /
+		x = kmemcheck_shadow_lookup(dst_addr);
+		if (x) {
+			kmemcheck_save_addr(dst_addr);
+			for (i = 0; i < size; ++i) {
+				x[i] = shadow[i];
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+			}
+		}
+	} else {
+		n = next_page - dst_addr;
+		BUG_ON(n > sizeof(shadow));
+
+		/ First page /
+		x = kmemcheck_shadow_lookup(dst_addr);
+		if (x) {
+			kmemcheck_save_addr(dst_addr);
+			for (i = 0; i < n; ++i) {
+				x[i] = shadow[i];
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+			}
+		}
+
+		/ Second page /
+		x = kmemcheck_shadow_lookup(next_page);
+		if (x) {
+			kmemcheck_save_addr(next_page);
+			for (i = n; i < size; ++i) {
+				x[i - n] = shadow[i];
+				shadow[i] = KMEMCHECK_SHADOW_INITIALIZED;
+			}
+		}
+	}
+
+	status = kmemcheck_shadow_test(shadow, size);
+	if (status == KMEMCHECK_SHADOW_INITIALIZED)
+		return;
+
+	if (kmemcheck_enabled)
+		kmemcheck_error_save(status, src_addr, size, regs);
+
+	if (kmemcheck_enabled == 2)
+		kmemcheck_enabled = 0;
+}
+
+enum kmemcheck_method {
+	KMEMCHECK_READ,
+	KMEMCHECK_WRITE,
+};
+
+static void kmemcheck_access(struct pt_regs *regs,
+	unsigned long fallback_address, enum kmemcheck_method fallback_method)
+{
+	const uint8_t *insn;
+	const uint8_t *insn_primary;
+	unsigned int size;
+
+	struct kmemcheck_context *data = &__get_cpu_var(kmemcheck_context);
+
+	/ Recursive fault -- ouch. /
+	if (data->busy) {
+		kmemcheck_show_addr(fallback_address);
+		kmemcheck_error_save_bug(regs);
+		return;
+	}
+
+	data->busy = true;
+
+	insn = (const uint8_t *) regs->ip;
+	insn_primary = kmemcheck_opcode_get_primary(insn);
+
+	kmemcheck_opcode_decode(insn, &size);
+
+	switch (insn_primary[0]) {
+#ifdef CONFIG_KMEMCHECK_BITOPS_OK
+		/ AND, OR, XOR /
+		*
+		 * Unfortunately, these instructions have to be excluded from
+		 * our regular checking since they access only some (and not
+		 * all) bits. This clears out "bogus" bitfield-access warnings.
+		 *
+	case 0x80:
+	case 0x81:
+	case 0x82:
+	case 0x83:
+		switch ((insn_primary[1] >> 3) & 7) {
+			/ OR *
+		case 1:
+			/ AND /
+		case 4:
+			/ XOR /
+		case 6:
+			kmemcheck_write(regs, fallback_address, size);
+			goto out;
+
+			/ ADD /
+		case 0:
+			/ ADC /
+		case 2:
+			/ SBB /
+		case 3:
+			/ SUB /
+		case 5:
+			/ CMP /
+		case 7:
+			break;
+		}
+		break;
+#endif
+
+		/ MOVS, MOVSB, MOVSW, MOVSD /
+	case 0xa4:
+	case 0xa5:
+		*
+		 * These instructions are special because they take two
+		 * addresses, but we only get one page fault.
+		 *
+		kmemcheck_copy(regs, regs->si, regs->di, size);
+		goto out;
+
+		/ CMPS, CMPSB, CMPSW, CMPSD /
+	case 0xa6:
+	case 0xa7:
+		kmemcheck_read(regs, regs->si, size);
+		kmemcheck_read(regs, regs->di, size);
+		goto out;
+	}
+
+	*
+	 * If the opcode isn't special in any way, we use the data from the
+	 * page fault handler to determine the address and type of memory
+	 * access.
+	 *
+	switch (fallback_method) {
+	case KMEMCHECK_READ:
+		kmemcheck_read(regs, fallback_address, size);
+		goto out;
+	case KMEMCHECK_WRITE:
+		kmemcheck_write(regs, fallback_address, size);
+		goto out;
+	}
+
+out:
+	data->busy = false;
+}
+ */
+
+/*
+bool kmemcheck_trap(struct pt_regs *regs)
+{
+	if (!kmemcheck_active(regs))
+		return false;
+
+	// We're done. 
+	kmemcheck_hide(regs);
+	return true;
+}
+*/
+
